{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4xR/Hzwv0v8YCjxYqV033",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Harshksaw/Notes/blob/main/LinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6hg0K__23pG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Regression Assignment\n",
        "\n",
        "# Q1. Simple Linear Regression vs. Multiple Linear Regression:\n",
        "Simple Linear Regression involves modeling the relationship between two variables, one independent variable, and one dependent variable. For example, predicting a person's weight (dependent) based on their height (independent).\n",
        "Multiple Linear Regression, on the other hand, includes multiple independent variables to predict a dependent variable. For instance, predicting a house's price (dependent) based on its size, number of bedrooms, and location (independents).\n",
        "\n",
        "# Q2. Assumptions of Linear Regression:\n",
        "Linear regression assumes:\n",
        "1. Linearity: The relationship between variables is linear.\n",
        "2. Independence: The residuals (errors) are independent.\n",
        "3. Homoscedasticity: Residuals have constant variance.\n",
        "4. Normality: Residuals are normally distributed.\n",
        "To check these assumptions, you can use scatterplots, residual plots, and statistical tests like the Shapiro-Wilk test for normality.\n",
        "\n",
        "# Q3. Interpreting Slope and Intercept:\n",
        "In a linear regression model (y = mx + b), the slope (m) represents the change in the dependent variable for a one-unit change in the independent variable. The intercept (b) is the predicted value of the dependent variable when the independent variable is zero. For example, in a salary prediction model, the slope could represent the increase in salary for each additional year of experience, and the intercept could be the estimated salary for someone with zero years of experience.\n",
        "\n",
        "# Q4. Gradient Descent:\n",
        "Gradient descent is an optimization algorithm used in machine learning to minimize the error or cost function of a model. It works by iteratively adjusting model parameters in the direction of steepest descent (negative gradient) to find the optimal parameter values that minimize the loss. It's used in training various machine learning models, including linear regression, by updating coefficients to fit the data.\n",
        "\n",
        "# Q5. Multiple Linear Regression Model:\n",
        "Multiple Linear Regression models the relationship between multiple independent variables and a single dependent variable. It's an extension of simple linear regression. The equation becomes: y = b0 + b1*x1 + b2*x2 + ... + bn*xn, where b0 is the intercept, b1, b2, etc., are coefficients, and x1, x2, etc., are independent variables.\n",
        "\n",
        "# Q6. Multicollinearity in Multiple Linear Regression:\n",
        "Multicollinearity occurs when independent variables in a multiple linear regression model are highly correlated, making it challenging to determine each variable's unique contribution to the dependent variable. You can detect multicollinearity using correlation matrices or variance inflation factors (VIFs). To address it, consider removing correlated variables or using techniques like principal component analysis (PCA).\n",
        "\n",
        "# Q7. Polynomial Regression:\n",
        "Polynomial regression is an extension of linear regression that models the relationship between variables as an nth-degree polynomial (quadratic, cubic, etc.) rather than a straight line. It can capture more complex relationships between variables, making it different from linear regression.\n",
        "\n",
        "# Q8. Advantages and Disadvantages of Polynomial Regression:\n",
        "Advantages:\n",
        "- Captures nonlinear relationships.\n",
        "- More flexible than linear regression.\n",
        "Disadvantages:\n",
        "- Can overfit data with high-degree polynomials.\n",
        "- Interpretability can be challenging.\n",
        "Use polynomial regression when you have evidence of a nonlinear relationship between variables, but be cautious of overfitting, especially with high-degree polynomials."
      ],
      "metadata": {
        "id": "DOqm6DFH246X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bqjfuF5r3nqF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}