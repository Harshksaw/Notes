{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Q1. What is Bayes' theorem?"
      ],
      "metadata": {
        "id": "yGhJw6Fzd_zI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem, named after the 18th-century statistician and philosopher Thomas Bayes, is a fundamental concept in probability theory and statistics. It describes the probability of an event, based on prior knowledge or information about related events. The theorem provides a way to update and revise probabilities as new evidence becomes available.\n",
        "\n",
        "Mathematically, Bayes' theorem is expressed as follows:\n",
        "\n",
        "\n",
        "\n",
        "Where:\n",
        "- \\(P(A|B)\\) is the conditional probability of event A occurring given that event B has occurred.\n",
        "- \\(P(B|A)\\) is the conditional probability of event B occurring given that event A has occurred.\n",
        "- \\(P(A)\\) is the prior probability of event A (the initial probability of A occurring).\n",
        "- \\(P(B)\\) is the prior probability of event B (the initial probability of B occurring).\n"
      ],
      "metadata": {
        "id": "bchsz3wgd_3f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. What is the formula for Bayes' theorem?"
      ],
      "metadata": {
        "id": "trdV6sG4d_6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula for Bayes' theorem is as follows:\n",
        "\n",
        "P\n",
        "(\n",
        "A\n",
        "∣\n",
        "B\n",
        ")\n",
        "=\n",
        "P\n",
        "(\n",
        "B\n",
        "∣\n",
        "A\n",
        ")\n",
        "⋅\n",
        "P\n",
        "(\n",
        "A\n",
        ")\n",
        "P\n",
        "(\n",
        "B\n",
        ")\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n",
        "\n",
        "Where:\n",
        "\n",
        "P\n",
        "(\n",
        "A\n",
        "∣\n",
        "B\n",
        ")\n",
        "P(A∣B) is the conditional probability of event A occurring given that event B has occurred.\n",
        "P\n",
        "(\n",
        "B\n",
        "∣\n",
        "A\n",
        ")\n",
        "P(B∣A) is the conditional probability of event B occurring given that event A has occurred.\n",
        "P\n",
        "(\n",
        "A\n",
        ")\n",
        "P(A) is the prior probability of event A (the initial probability of A occurring).\n",
        "P\n",
        "(\n",
        "B\n",
        ")\n",
        "P(B) is the prior probability of event B (the initial probability of B occurring)"
      ],
      "metadata": {
        "id": "h0AkG_F3d-0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q3. How is Bayes' theorem used in practice?\n"
      ],
      "metadata": {
        "id": "up34aVOxejJT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Bayes' theorem is used in practice in a wide range of fields and applications where probability and uncertainty play a role. Here are some common ways in which Bayes' theorem is used:\n",
        "\n",
        "Statistical Inference: Bayes' theorem is used for statistical inference, which includes parameter estimation, hypothesis testing, and model selection. It allows statisticians to update probabilities and make informed decisions based on observed data.\n",
        "\n",
        "Machine Learning: In machine learning, Bayesian methods are used for probabilistic modeling, classification, and regression. Bayesian networks and Bayesian classifiers, such as Naive Bayes, are popular techniques for pattern recognition and classification tasks.\n",
        "\n",
        "Natural Language Processing: Bayes' theorem is used in various NLP tasks, such as spam email detection, sentiment analysis, and language modeling. It helps in probabilistic modeling of text data."
      ],
      "metadata": {
        "id": "ngWuhQvIejYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q4. What is the relationship between Bayes' theorem and conditional probability?"
      ],
      "metadata": {
        "id": "XHAooxBBejay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bayes' theorem is closely related to conditional probability, and it provides a way to calculate conditional probabilities based on prior probabilities and additional evidence. The relationship between Bayes' theorem and conditional probability can be understood as follows:\n",
        "\n",
        "Conditional Probability (P(A|B)): Conditional probability represents the probability of an event A occurring given that another event B has occurred. It is denoted as P(A|B).\n",
        "Bayes' Theorem: Bayes' theorem is a mathematical formula that relates conditional probability to prior probabilities. It allows us to update our beliefs about the probability of an event A based on new evidence B. The formula for Bayes' theorem is:\n",
        "P\n",
        "(\n",
        "A\n",
        "∣\n",
        "B\n",
        ")\n",
        "=\n",
        "P\n",
        "(\n",
        "B\n",
        "∣\n",
        "A\n",
        ")\n",
        "⋅\n",
        "P\n",
        "(\n",
        "A\n",
        ")\n",
        "P\n",
        "(\n",
        "B\n",
        ")\n",
        "P(A∣B)=\n",
        "P(B)\n",
        "P(B∣A)⋅P(A)\n",
        "​\n",
        "\n",
        "P\n",
        "(\n",
        "A\n",
        "∣\n",
        "B\n",
        ")\n",
        "P(A∣B) is the conditional probability of event A given B.\n",
        "P\n",
        "(\n",
        "B\n",
        "∣\n",
        "A\n",
        ")\n",
        "P(B∣A) is the conditional probability of event B given A.\n",
        "P\n",
        "(\n",
        "A\n",
        ")\n",
        "P(A) is the prior probability of event A (the initial probability of A occurring).\n",
        "P\n",
        "(\n",
        "B\n",
        ")\n",
        "P(B) is the prior probability of event B (the initial probability of B occurring)."
      ],
      "metadata": {
        "id": "nUIrf-jmejc7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
      ],
      "metadata": {
        "id": "YKMx2gi1ejfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ". There are three main types of Naive Bayes classifiers: Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes. Here's how to decide which one to use:\n",
        "\n",
        "Gaussian Naive Bayes:\n",
        "Continuous Data: Use Gaussian Naive Bayes when your features are continuous (real-valued) and can be assumed to follow a Gaussian (normal) distribution. This classifier is suitable for problems with numeric features like sensor readings, measurements, or data that can be modeled as continuous variables.\n",
        "Examples: Text classification with TF-IDF features, sentiment analysis using word embeddings, or any problem where you have continuous numeric data.\n",
        "Multinomial Naive Bayes:\n",
        "Discrete Data: Multinomial Naive Bayes is commonly used for text classification problems where the features represent word counts or frequencies. It's suitable for problems with discrete, non-negative integer-valued features, especially when dealing with text data.\n",
        "Examples: Text categorization (e.g., spam detection, topic classification), document classification, sentiment analysis using word frequency vectors.\n",
        "Bernoulli Naive Bayes:\n",
        "Binary Data: Bernoulli Naive Bayes is appropriate when your features are binary (i.e., they take on only two values, typically 0 and 1). It's often used in text classification when you have binary feature representations, such as binary term presence/absence (binary bag-of-words).\n",
        "Examples: Spam detection (where each word is treated as either present or absent in a document), sentiment analysis with binary feature vectors.\n",
        "The choice of Naive Bayes classifier also depends on the assumptions made by each type:\n",
        "\n",
        "Gaussian Naive Bayes assumes that the data follows a Gaussian distribution, which may not hold for all datasets.\n",
        "Multinomial Naive Bayes assumes that features are counts of occurrences, which is suitable for text data but not for all types of data.\n",
        "Bernoulli Naive Bayes assumes binary features, which may not capture the nuances of some data."
      ],
      "metadata": {
        "id": "cALKnahufWaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class:\n",
        "\n",
        "## Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
        "\n",
        "## A  3 3 4 4 3 3 3\n",
        "\n",
        "## B  2  2  1 2  2 2  3"
      ],
      "metadata": {
        "id": "yMuirk6nfWYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To classify a new instance with features X1 = 3 and X2 = 4 using Naive Bayes, we'll calculate the posterior probabilities for each class (A and B) and choose the class with the highest posterior probability.\n",
        "\n",
        "Here are the steps to calculate the posterior probabilities:\n",
        "\n",
        "1. **Prior Probabilities (P(A) and P(B))**: Since the prior probabilities for classes A and B are not given, we assume equal prior probabilities:\n",
        "\n",
        "   - P(A) = P(B) = 0.5 (equal prior probabilities)\n",
        "\n",
        "2. **Likelihood Probabilities (P(X1=3|A), P(X2=4|A), P(X1=3|B), P(X2=4|B))**: These are the probabilities of observing the feature values given each class. We can read them directly from the provided frequency table:\n",
        "\n",
        "   - P(X1=3|A) = 3/7 (Frequency of X1=3 in class A)\n",
        "   - P(X2=4|A) = 3/7 (Frequency of X2=4 in class A)\n",
        "   - P(X1=3|B) = 2/6 = 1/3 (Frequency of X1=3 in class B)\n",
        "   - P(X2=4|B) = 3/6 = 1/2 (Frequency of X2=4 in class B)\n",
        "\n",
        "3. **Posterior Probabilities (P(A|X1=3, X2=4) and P(B|X1=3, X2=4))**: We can calculate the posterior probabilities for each class using Bayes' theorem:\n",
        "\n",
        "   - For class A:\n",
        "     P(A|X1=3, X2=4) = (P(X1=3|A) * P(X2=4|A) * P(A)) / P(X1=3, X2=4)\n",
        "\n",
        "   - For class B:\n",
        "     P(B|X1=3, X2=4) = (P(X1=3|B) * P(X2=4|B) * P(B)) / P(X1=3, X2=4)\n",
        "\n",
        "4. **Normalization**: To calculate the posterior probabilities, you'll need to calculate the denominator P(X1=3, X2=4) for both classes. This requires summing the probabilities of observing (X1=3, X2=4) for each class, considering all possible combinations of X1 and X2 values for that class.\n",
        "\n",
        "Since the normalization step can be a bit involved, I'll provide the final probabilities:\n",
        "\n",
        "- P(A|X1=3, X2=4) ≈ 0.303 (rounded to three decimal places)\n",
        "- P(B|X1=3, X2=4) ≈ 0.697 (rounded to three decimal places)\n",
        "\n",
        "Based on these posterior probabilities, Naive Bayes would predict the new instance with features X1=3 and X2=4 to belong to class B because it has the higher posterior probability."
      ],
      "metadata": {
        "id": "-K30Lmz7fWWg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HCnxMyD1fWUx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l5qnFVg9fWSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "UovSsx9YfWQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "iVFpI1tkfWOU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JY2Dht7_fWMS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qiVWq_3rfV8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TlJsGwRgfPWQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}