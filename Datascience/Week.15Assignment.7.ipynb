{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f17c4f35-8eca-4e92-9f9b-a84a2714c097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Week.15 \n",
    "#Assignment.7 \n",
    "#Question.1 : Explain the difference between linear regression and logistic regression models. Provide an example of\n",
    "#a scenario where logistic regression would be more appropriate.\n",
    "#Answer.1 : # Difference Between Linear Regression and Logistic Regression:\n",
    "\n",
    "# 1. **Nature of Dependent Variable:**\n",
    "#    - Linear Regression: Dependent variable is continuous, representing a quantity.\n",
    "#    - Logistic Regression: Dependent variable is binary or categorical, representing classes or categories.\n",
    "\n",
    "# 2. **Output Range:**\n",
    "#    - Linear Regression: Output can take any real value within a range, unbounded.\n",
    "#    - Logistic Regression: Output is a probability score between 0 and 1, representing likelihood of belonging to\n",
    "#a specific class.\n",
    "\n",
    "# 3. **Equation:**\n",
    "#    - Linear Regression: Equation is y = mx + b, where y is the dependent variable, x is the independent variable, m \n",
    "#is the slope, and b is the intercept.\n",
    "#    - Logistic Regression: Equation uses the logistic function (sigmoid), P(Y=1) = 1 / (1 + e^-(mx + b)), \n",
    "#where P(Y=1) is the probability of the positive class.\n",
    "\n",
    "# 4. **Objective:**\n",
    "#    - Linear Regression: Models the relationship between the dependent variable and one or more independent \n",
    "#variables by fitting a linear equation.\n",
    "#    - Logistic Regression: Designed for binary or multiclass classification problems, models the probability that\n",
    "#an instance belongs to a particular category.\n",
    "\n",
    "# Scenario Example for Logistic Regression:\n",
    "\n",
    "# Consider a scenario of predicting whether a student passes or fails an exam based on the number of hours spent\n",
    "#studying.\n",
    "# - Dependent Variable: Pass/Fail (Binary)\n",
    "# - Independent Variable: Hours Studied\n",
    "# Logistic Regression is more appropriate for this scenario due to the binary nature of the outcome.\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Generate sample data\n",
    "data = {'Hours_Studied': [2, 3, 5, 1, 7, 8, 4, 6],\n",
    "        'Pass_Exam': [0, 0, 1, 0, 1, 1, 0, 1]}  # 0: Fail, 1: Pass\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df[['Hours_Studied']]\n",
    "y = df['Pass_Exam']\n",
    "\n",
    "# Initialize logistic regression model\n",
    "logistic_model = LogisticRegression()\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the logistic regression model\n",
    "logistic_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = logistic_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f22ae8e-6f7e-4d48-82d8-c34779199323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 :  What is the cost function used in logistic regression, and how is it optimized?\n",
    "#Answer.2 : # Cost Function in Logistic Regression and Optimization:\n",
    "\n",
    "# 1. **Logistic Regression Cost Function (Binary Classification):**\n",
    "#    - The cost function in logistic regression is the logistic loss or cross-entropy loss.\n",
    "#    - It is used to measure the difference between the predicted probabilities and the actual binary outcomes.\n",
    "\n",
    "# 2. **Logistic Loss (Binary Classification):**\n",
    "#    - For a single training example (x, y), the logistic loss is defined as:\n",
    "#      - J(θ) = - [y * log(h(x)) + (1 - y) * log(1 - h(x))]\n",
    "#      - where h(x) is the sigmoid function output, representing the predicted probability.\n",
    "\n",
    "# 3. **Objective (Minimization):**\n",
    "#    - The goal is to minimize the logistic loss across all training examples.\n",
    "#    - Minimizing the loss improves the model's ability to accurately predict the class probabilities.\n",
    "\n",
    "# 4. **Optimization Algorithm:**\n",
    "#    - Gradient Descent is commonly used to minimize the cost function and find the optimal parameters (θ) of the \n",
    "#logistic regression model.\n",
    "\n",
    "# 5. **Gradient Descent Steps:**\n",
    "#    - Update θ iteratively using the partial derivatives of the cost function with respect to each parameter.\n",
    "#    - Repeat until convergence or a predefined number of iterations:\n",
    "#        - θj := θj - α * ∂J(θ) / ∂θj   (for each parameter θj)\n",
    "#        - where α is the learning rate, controlling the step size.\n",
    "\n",
    "# 6. **Vectorized Form:**\n",
    "#    - The vectorized form of the update rule for all parameters can be expressed as:\n",
    "#        - θ := θ - α * (∂J(θ) / ∂θ)\n",
    "#        - where ∂J(θ) / ∂θ is the gradient vector.\n",
    "\n",
    "# 7. **Regularization (Optional):**\n",
    "#    - Regularization terms (L1 or L2) can be added to the cost function to prevent overfitting.\n",
    "#    - Regularization is controlled by a regularization parameter (λ).\n",
    "\n",
    "# Example in Python (using scikit-learn):\n",
    "#   - The logistic regression model in scikit-learn automatically optimizes the cost function using an optimization \n",
    "#algorithm (typically variants of gradient descent).\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# model = LogisticRegression()\n",
    "# model.fit(X_train, y_train)  # X_train: input features, y_train: target variable (binary)\n",
    "\n",
    "# Note: The actual optimization details (e.g., specific gradient descent variant) may vary based on the implementation\n",
    "#and library used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "336abc8b-5485-4fdc-9450-4ba4d6be72bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : Explain the concept of regularization in logistic regression and how it helps prevent overfitting.\n",
    "#Answer.3 : # Regularization in Logistic Regression and Overfitting Prevention:\n",
    "\n",
    "# 1. **Regularization in Logistic Regression:**\n",
    "#    - Regularization is a technique used to prevent overfitting by adding a penalty term to the logistic regression \n",
    "#cost function.\n",
    "#    - It discourages the model from fitting the training data too closely and helps generalize better to unseen data.\n",
    "\n",
    "# 2. **Cost Function with Regularization (L2 Regularization):**\n",
    "#    - The regularized cost function for logistic regression (L2 regularization) is:\n",
    "#        - J(θ) = - [y * log(h(x)) + (1 - y) * log(1 - h(x))] + λ/2m * ∑(θj^2)\n",
    "#        - The additional term is the regularization term, where λ is the regularization parameter, m is the\n",
    "#number of training examples, and θj are the model parameters.\n",
    "\n",
    "# 3. **Objective with Regularization:**\n",
    "#    - The goal is to minimize the regularized cost function.\n",
    "#    - The regularization term penalizes large parameter values, leading to a more balanced model.\n",
    "\n",
    "# 4. **Types of Regularization:**\n",
    "#    - L1 Regularization: Adds the absolute values of the parameters to the cost function.\n",
    "#        - J(θ) = - [y * log(h(x)) + (1 - y) * log(1 - h(x))] + λ/m * ∑|θj|\n",
    "#    - L2 Regularization (commonly used): Adds the squared values of the parameters to the cost function.\n",
    "#        - J(θ) = - [y * log(h(x)) + (1 - y) * log(1 - h(x))] + λ/2m * ∑(θj^2)\n",
    "\n",
    "# 5. **Effect on Model Complexity:**\n",
    "#    - Regularization acts as a constraint on the model, penalizing overly complex models.\n",
    "#    - It encourages the model to use simpler decision boundaries, reducing the risk of overfitting.\n",
    "\n",
    "# 6. **Choosing the Regularization Parameter (λ):**\n",
    "#    - The regularization parameter (λ) controls the strength of regularization.\n",
    "#    - Tuning λ involves finding a balance between fitting the training data well and preventing overfitting.\n",
    "#    - Cross-validation is commonly used to determine an optimal value for λ.\n",
    "\n",
    "# 7. **Implementation in scikit-learn:**\n",
    "#    - In scikit-learn, the regularization parameter for logistic regression is denoted by 'C' (inverse of \n",
    "#regularization strength).\n",
    "#    - Smaller values of 'C' correspond to stronger regularization.\n",
    "\n",
    "# Example in Python (using scikit-learn):\n",
    "#   - Logistic Regression with L2 regularization:\n",
    "#     ```\n",
    "#     from sklearn.linear_model import LogisticRegression\n",
    "#     model = LogisticRegression(C=1.0)  # Adjust C for regularization strength\n",
    "#     model.fit(X_train, y_train)\n",
    "#     ```\n",
    "\n",
    "# Note: Regularization is a crucial tool for preventing overfitting, and the choice of regularization strength is \n",
    "#often determined through experimentation and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a88839e-eb4f-4413-968b-da47e5243dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : What is the ROC curve, and how is it used to evaluate the performance of the logistic regression\n",
    "#model?\n",
    "#Answer.4 : # ROC Curve and Evaluation of Logistic Regression Model:\n",
    "\n",
    "# 1. **ROC Curve (Receiver Operating Characteristic):**\n",
    "#    - The ROC curve is a graphical representation of the trade-off between true positive rate (sensitivity) and \n",
    "#false positive rate (1 - specificity) for different threshold values.\n",
    "#    - It is used to assess the performance of classification models, including logistic regression.\n",
    "\n",
    "# 2. **True Positive Rate (Sensitivity):**\n",
    "#    - The true positive rate is the proportion of actual positive instances correctly predicted by the model.\n",
    "#    - TPR = True Positives / (True Positives + False Negatives)\n",
    "\n",
    "# 3. **False Positive Rate (1 - Specificity):**\n",
    "#    - The false positive rate is the proportion of actual negative instances incorrectly predicted as positive by the \n",
    "#model.\n",
    "#    - FPR = False Positives / (False Positives + True Negatives)\n",
    "\n",
    "# 4. **Area Under the ROC Curve (AUC-ROC):**\n",
    "#    - AUC-ROC represents the area under the ROC curve and provides a single scalar value to quantify the model's\n",
    "#discriminative power.\n",
    "#    - AUC-ROC values range from 0 to 1, with higher values indicating better model performance.\n",
    "\n",
    "# 5. **Interpretation of ROC Curve:**\n",
    "#    - An ideal model has an ROC curve that hugs the top-left corner, resulting in a larger AUC-ROC.\n",
    "#    - The diagonal line (45-degree line) represents random chance, and the goal is for the ROC curve to be above \n",
    "#this line.\n",
    "\n",
    "# 6. **Choosing the Threshold:**\n",
    "#    - The ROC curve is generated by varying the classification threshold.\n",
    "#    - The choice of threshold depends on the desired balance between sensitivity and specificity, considering the\n",
    "#specific use case.\n",
    "\n",
    "# 7. **Implementation in scikit-learn:**\n",
    "#    - In scikit-learn, the `roc_curve` function is used to compute the ROC curve, and the `roc_auc_score`\n",
    "#function calculates AUC-ROC.\n",
    "\n",
    "# Example in Python (using scikit-learn):\n",
    "#   ```\n",
    "#   from sklearn.metrics import roc_curve, roc_auc_score\n",
    "#   import matplotlib.pyplot as plt\n",
    "\n",
    "#   # Assuming y_true and y_probs are the true labels and predicted probabilities, respectively\n",
    "#   fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
    "#   auc_score = roc_auc_score(y_true, y_probs)\n",
    "\n",
    "#   # Plot the ROC curve\n",
    "#   plt.figure(figsize=(8, 8))\n",
    "#   plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "#   plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "#   plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "#   plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "#   plt.title('ROC Curve')\n",
    "#   plt.legend()\n",
    "#   plt.show()\n",
    "#   ```\n",
    "\n",
    "# Note: The ROC curve provides insights into the model's performance across different thresholds, helping to make \n",
    "#informed decisions based on the desired balance between true positives and false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba63aebf-0f10-47b5-8140-a1fdfa5f0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Qustion.5 : What are some common techniques for feature selection in logistic regression? How do these\n",
    "#techniques help improve the model's performance?\n",
    "#Answer.5 : # Feature Selection in Logistic Regression:\n",
    "\n",
    "# 1. **Why Feature Selection:**\n",
    "#    - Feature selection is crucial to improve model performance by selecting the most relevant features and\n",
    "#avoiding overfitting.\n",
    "#    - It reduces dimensionality, enhances interpretability, and may lead to faster training.\n",
    "\n",
    "# 2. **Common Techniques for Feature Selection in Logistic Regression:**\n",
    "\n",
    "#    a. **Univariate Feature Selection:**\n",
    "#       - Evaluate each feature's relationship with the target variable independently.\n",
    "#       - Methods include chi-squared test, ANOVA, and mutual information.\n",
    "#       - Select features based on statistical significance.\n",
    "\n",
    "#    b. **Recursive Feature Elimination (RFE):**\n",
    "#       - Iteratively fits the model and eliminates the least important feature.\n",
    "#       - Continues until the desired number of features is reached.\n",
    "#       - RFE relies on model performance metrics for feature ranking.\n",
    "\n",
    "#    c. **L1 Regularization (Lasso):**\n",
    "#       - Introduces sparsity by penalizing the absolute values of feature coefficients.\n",
    "#       - Encourages some coefficients to become exactly zero.\n",
    "#       - Features with non-zero coefficients are selected.\n",
    "\n",
    "#    d. **Tree-based Methods:**\n",
    "#       - Utilize decision trees or ensemble methods (e.g., Random Forest, Gradient Boosting).\n",
    "#       - Feature importance scores are obtained, and less important features are pruned.\n",
    "#       - Can handle non-linear relationships.\n",
    "\n",
    "#    e. **Information Gain or Mutual Information:**\n",
    "#       - Measures the reduction in uncertainty about the target variable given the knowledge of a feature.\n",
    "#       - Useful for both categorical and continuous features.\n",
    "#       - Higher information gain suggests better predictive power.\n",
    "\n",
    "# 3. **How These Techniques Improve Performance:**\n",
    "\n",
    "#    a. **Reduces Overfitting:**\n",
    "#       - By selecting only relevant features, the model is less likely to fit noise in the data.\n",
    "#       - Reducing overfitting improves generalization to unseen data.\n",
    "\n",
    "#    b. **Enhances Interpretability:**\n",
    "#       - Models with fewer features are easier to interpret and understand.\n",
    "#       - Simplifying the model may reveal more meaningful relationships.\n",
    "\n",
    "#    c. **Computational Efficiency:**\n",
    "#       - Training and inference are faster with fewer features.\n",
    "#       - Particularly beneficial for large datasets or real-time applications.\n",
    "\n",
    "#    d. **Handles Multicollinearity:**\n",
    "#       - Removing highly correlated features helps mitigate multicollinearity issues.\n",
    "#       - Logistic regression assumes features are not perfectly correlated.\n",
    "\n",
    "# 4. **Implementation in scikit-learn:**\n",
    "#    - scikit-learn provides implementations for many feature selection techniques.\n",
    "#    - For example, `SelectKBest` for univariate selection, `RFE` for recursive feature elimination, and\n",
    "#`SelectFromModel` for L1 regularization.\n",
    "\n",
    "# Note: The choice of feature selection technique depends on the dataset, problem complexity, and the desired \n",
    "#characteristics of the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16f2a9ee-adf1-43cf-83ae-183c8de51690",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : How can you handle imbalanced datasets in logistic regression? What are some strategies for dealing\n",
    "#with class imbalance?\n",
    "#Answer.6 : # Handling Imbalanced Datasets in Logistic Regression:\n",
    "\n",
    "# 1. **Why Imbalanced Datasets are Challenging:**\n",
    "#    - Imbalanced datasets have significantly unequal distribution of classes, leading to biased models.\n",
    "#    - In logistic regression, this imbalance may result in poor performance, especially for the minority class.\n",
    "\n",
    "# 2. **Common Strategies for Dealing with Class Imbalance:**\n",
    "\n",
    "#    a. **Resampling Techniques:**\n",
    "#       - **Oversampling:** Increase the number of instances in the minority class by duplicating or generating \n",
    "#synthetic samples.\n",
    "#         ```\n",
    "#         from imblearn.over_sampling import RandomOverSampler\n",
    "#         ros = RandomOverSampler(random_state=42)\n",
    "#         X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "#         ```\n",
    "#       - **Undersampling:** Reduce the number of instances in the majority class by randomly removing samples.\n",
    "#         ```\n",
    "#         from imblearn.under_sampling import RandomUnderSampler\n",
    "#         rus = RandomUnderSampler(random_state=42)\n",
    "#         X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "#         ```\n",
    "\n",
    "#    b. **Synthetic Data Generation:**\n",
    "#       - Techniques like SMOTE (Synthetic Minority Over-sampling Technique) generate synthetic samples for the\n",
    "#minority class.\n",
    "#         ```\n",
    "#         from imblearn.over_sampling import SMOTE\n",
    "#         smote = SMOTE(random_state=42)\n",
    "#         X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "#         ```\n",
    "\n",
    "#    c. **Weighted Classes:**\n",
    "#       - Assign different weights to classes to give more importance to the minority class during training.\n",
    "#         ```\n",
    "#         from sklearn.utils.class_weight import compute_class_weight\n",
    "#         class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "#         model = LogisticRegression(class_weight=dict(zip(np.unique(y), class_weights)))\n",
    "#         ```\n",
    "\n",
    "#    d. **Cost-sensitive Learning:**\n",
    "#       - Introduce misclassification costs to penalize errors on the minority class more severely.\n",
    "#         ```\n",
    "#         model = LogisticRegression(class_weight='balanced', C=0.5)\n",
    "#         ```\n",
    "\n",
    "#    e. **Ensemble Methods:**\n",
    "#       - Use ensemble methods like Random Forest with balanced class weights or boosting algorithms.\n",
    "#         ```\n",
    "#         from sklearn.ensemble import RandomForestClassifier\n",
    "#         model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "#         ```\n",
    "\n",
    "#    f. **Evaluation Metrics:**\n",
    "#       - Instead of accuracy, consider using precision, recall, F1-score, or area under the ROC curve (AUC-ROC) \n",
    "#to evaluate model performance.\n",
    "\n",
    "# 3. **Implementation in scikit-learn and imbalanced-learn:**\n",
    "#    - `imbalanced-learn` is a useful library for dealing with imbalanced datasets in scikit-learn.\n",
    "#    - Install it using: `pip install imbalanced-learn`\n",
    "\n",
    "# Note: The choice of strategy depends on the specific characteristics of the dataset and the problem at hand.\n",
    "#Experimentation and validation are crucial for finding the most effective approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c009b2a-6ae9-4a79-b57a-ee5045177bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : Can you discuss some common issues and challenges that may arise when implementing logistic\n",
    "#regression, and how they can be addressed? For example, what can be done if there is multicollinearity\n",
    "#among the independent variables?\n",
    "#Answer.7 : # Common Issues and Challenges in Logistic Regression:\n",
    "\n",
    "# 1. **Multicollinearity:**\n",
    "#    - **Issue:** High correlation among independent variables can lead to multicollinearity.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **VIF (Variance Inflation Factor):** Calculate VIF for each variable and remove or combine highly \n",
    "#correlated variables.\n",
    "#        ```\n",
    "#        from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "#        def calculate_vif(data):\n",
    "#            vif_data = pd.DataFrame()\n",
    "#            vif_data[\"Variable\"] = data.columns\n",
    "#            vif_data[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "#            return vif_data\n",
    "\n",
    "#        # Check VIF for multicollinearity\n",
    "#        vif_results = calculate_vif(X)\n",
    "#        ```\n",
    "\n",
    "# 2. **Outliers:**\n",
    "#    - **Issue:** Outliers can influence the logistic regression model.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Detection and Removal:** Identify and remove outliers using statistical methods or visualization.\n",
    "#        ```\n",
    "#        from scipy.stats import zscore\n",
    "\n",
    "#        # Calculate z-scores and remove outliers\n",
    "#        z_scores = zscore(X)\n",
    "#        X_no_outliers = X[(z_scores < 3).all(axis=1)]\n",
    "#        ```\n",
    "\n",
    "# 3. **Imbalanced Datasets:**\n",
    "#    - **Issue:** Unequal distribution of classes may lead to biased models.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Resampling Techniques:** Oversampling, undersampling, or synthetic data generation.\n",
    "#        ```\n",
    "#        from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "#        ros = RandomOverSampler(random_state=42)\n",
    "#        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "#        ```\n",
    "\n",
    "# 4. **Feature Scaling:**\n",
    "#    - **Issue:** Logistic regression is sensitive to the scale of features.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Standardization or Normalization:** Scale features to have similar ranges.\n",
    "#        ```\n",
    "#        from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#        scaler = StandardScaler()\n",
    "#        X_scaled = scaler.fit_transform(X)\n",
    "#        ```\n",
    "\n",
    "# 5. **Model Overfitting:**\n",
    "#    - **Issue:** Overfitting may occur, especially with complex models.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Regularization:** Introduce L1 or L2 regularization to penalize large coefficients.\n",
    "#        ```\n",
    "#        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#        model = LogisticRegression(penalty='l2', C=1.0)\n",
    "#        ```\n",
    "\n",
    "# 6. **Choice of Evaluation Metrics:**\n",
    "#    - **Issue:** Accuracy may not be sufficient for imbalanced datasets.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Precision, Recall, F1-Score, AUC-ROC:** Choose metrics that are more informative about model performance.\n",
    "#        ```\n",
    "#        from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "#        ```\n",
    "\n",
    "# 7. **Sample Size:**\n",
    "#    - **Issue:** Logistic regression may require a sufficiently large sample size.\n",
    "#    - **Addressing Strategy:**\n",
    "#      - **Ensure an Adequate Sample Size:** Aim for a sample size that provides statistical power for the analysis.\n",
    "\n",
    "# Note: Each issue may have multiple strategies, and the choice depends on the specific characteristics of the \n",
    "#dataset and the problem at hand. Experimentation and validation are key.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
