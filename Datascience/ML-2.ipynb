{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) in Machine Learning\n",
    "Q1: Mathematical Formula for a Linear SVM\n",
    "In a linear SVM, the decision boundary is represented by a hyperplane. The mathematical formula for a linear SVM can be expressed as:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "w\n",
    "⋅\n",
    "x\n",
    "+\n",
    "�\n",
    "f(x)=w⋅x+b\n",
    "\n",
    "Here:\n",
    "\n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "f(x) is the decision function.\n",
    "w\n",
    "w is the weight vector.\n",
    "x\n",
    "x is the input feature vector.\n",
    "�\n",
    "b is the bias term.\n",
    "Q2: Objective Function of a Linear SVM\n",
    "The objective function of a linear SVM aims to maximize the margin between different classes while minimizing classification errors. The objective function is expressed as:\n",
    "\n",
    "Minimize \n",
    "1\n",
    "2\n",
    "∥\n",
    "w\n",
    "∥\n",
    "2\n",
    "+\n",
    "�\n",
    "∑\n",
    "�\n",
    "=\n",
    "1\n",
    "�\n",
    "�\n",
    "�\n",
    "Minimize  \n",
    "2\n",
    "1\n",
    "​\n",
    " ∥w∥ \n",
    "2\n",
    " +C∑ \n",
    "i=1\n",
    "n\n",
    "​\n",
    " ξ \n",
    "i\n",
    "​\n",
    " \n",
    "\n",
    "Subject to the constraints:\n",
    "�\n",
    "�\n",
    "(\n",
    "w\n",
    "⋅\n",
    "x\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "≥\n",
    "1\n",
    "−\n",
    "�\n",
    "�\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)≥1−ξ \n",
    "i\n",
    "​\n",
    " \n",
    "�\n",
    "�\n",
    "≥\n",
    "0\n",
    "ξ \n",
    "i\n",
    "​\n",
    " ≥0\n",
    "\n",
    "Here:\n",
    "\n",
    "∥\n",
    "w\n",
    "∥\n",
    "2\n",
    "∥w∥ \n",
    "2\n",
    "  represents the magnitude of the weight vector.\n",
    "�\n",
    "C is the regularization parameter controlling the trade-off between achieving a low training error and a large margin.\n",
    "�\n",
    "�\n",
    "ξ \n",
    "i\n",
    "​\n",
    "  are slack variables that allow for the existence of misclassifications.\n",
    "Q3: Kernel Trick in SVM\n",
    "The kernel trick in SVM allows the algorithm to implicitly map the input space into a higher-dimensional feature space without explicitly calculating the transformation. Kernels are functions that compute the dot product of the transformed features in this higher-dimensional space. Common kernels include polynomial, radial basis function (RBF), and sigmoid.\n",
    "\n",
    "Q4: Role of Support Vectors in SVM\n",
    "Support vectors are the data points that lie closest to the decision boundary (margin). They play a crucial role in defining the decision boundary and the margin. In the case of a linear SVM, support vectors are the points that satisfy the equation \n",
    "�\n",
    "�\n",
    "(\n",
    "w\n",
    "⋅\n",
    "x\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)=1. These are the critical points that determine the orientation of the hyperplane.\n",
    "\n",
    "Example: Consider a dataset with two classes, and the support vectors are the instances at the boundary between the classes. These support vectors heavily influence the position and orientation of the decision boundary.\n",
    "\n",
    "Q5: Illustration with Examples and Graphs of Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM\n",
    "Hyperplane:\n",
    "A hyperplane is the decision boundary that separates different classes. In a 2D space, it is a line; in 3D, it is a plane. The equation for a hyperplane in a 2D space is \n",
    "�\n",
    "(\n",
    "�\n",
    ")\n",
    "=\n",
    "w\n",
    "⋅\n",
    "x\n",
    "+\n",
    "�\n",
    "=\n",
    "0\n",
    "f(x)=w⋅x+b=0.\n",
    "\n",
    "Marginal Plane:\n",
    "The marginal plane is the region that includes the hyperplane and the support vectors. It determines the width of the margin. Points on the marginal plane have \n",
    "�\n",
    "�\n",
    "(\n",
    "w\n",
    "⋅\n",
    "x\n",
    "�\n",
    "+\n",
    "�\n",
    ")\n",
    "=\n",
    "1\n",
    "y \n",
    "i\n",
    "​\n",
    " (w⋅x \n",
    "i\n",
    "​\n",
    " +b)=1.\n",
    "\n",
    "Soft Margin and Hard Margin:\n",
    "Hard Margin SVM: It strictly enforces that all instances must be outside the margin. Suitable for well-separated data, but sensitive to outliers.\n",
    "\n",
    "Soft Margin SVM: Allows for some instances to be inside the margin or even on the wrong side of the hyperplane. It introduces slack variables \n",
    "�\n",
    "�\n",
    "ξ \n",
    "i\n",
    "​\n",
    "  to allow for margin violations. Suitable for data with noise or outliers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree Classifier\n",
    "Q1: Description of Decision Tree Classifier Algorithm\n",
    "The Decision Tree Classifier is a supervised machine learning algorithm used for both classification and regression tasks. It works by recursively partitioning the dataset into subsets based on the most significant attribute at each node. The goal is to create a tree-like model that predicts the target variable by making decisions at each internal node and assigning a label at each leaf node.\n",
    "\n",
    "Q2: Step-by-Step Explanation of Mathematical Intuition\n",
    "The decision tree algorithm follows these steps:\n",
    "\n",
    "Entropy Calculation: Measure of impurity in the dataset.\n",
    "Information Gain Calculation: Determine the effectiveness of a feature in reducing entropy.\n",
    "Select the Best Feature: Choose the feature with the highest information gain as the decision node.\n",
    "Recursive Partitioning: Split the dataset into subsets based on the chosen feature.\n",
    "Repeat: Continue the process for each subset until a stopping condition is met.\n",
    "Q3: Using Decision Tree for Binary Classification\n",
    "In binary classification, the decision tree predicts one of two classes at each leaf node, making it suitable for problems like spam detection (spam or not spam), fraud detection (fraudulent or not), etc.\n",
    "\n",
    "Q4: Geometric Intuition and Predictions\n",
    "Decision trees can be visualized as a series of splits in feature space. Each split creates boundaries that separate data points belonging to different classes. Predictions are made by traversing the tree from the root to a leaf, where the final decision is based on the majority class in that leaf.\n",
    "\n",
    "Q5: Confusion Matrix and Model Evaluation\n",
    "The confusion matrix is a table that describes the performance of a classification model. It includes metrics such as True Positives (TP), True Negatives (TN), False Positives (FP), and False Negatives (FN).\n",
    "\n",
    "Q6: Example of Confusion Matrix and Metrics Calculation\n",
    "Predicted Positive\tPredicted Negative\n",
    "Actual Positive\tTrue Positive (TP)\tFalse Negative (FN)\n",
    "Actual Negative\tFalse Positive (FP)\tTrue Negative (TN)\n",
    "Precision = TP / (TP + FP)\n",
    "\n",
    "Recall = TP / (TP + FN)\n",
    "\n",
    "F1 Score = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Q7: Importance of Choosing Evaluation Metric\n",
    "Choosing the right evaluation metric is crucial as it depends on the specific goals and requirements of the problem. For example, in medical diagnosis, where false negatives can be critical, recall might be more important than precision.\n",
    "\n",
    "Q8: Example Where Precision is Most Important\n",
    "In an email filtering system, where marking a non-spam email as spam is more detrimental than missing some spam emails, precision is crucial.\n",
    "\n",
    "Q9: Example Where Recall is Most Important\n",
    "In a cancer detection system, missing a positive case (patient having cancer) is more critical than incorrectly diagnosing a healthy patient. Thus, recall is more important.\n",
    "\n",
    "Feel free to customize and expand on these answers based on your specific context and requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: SVM Implementation through Iris dataset.\n",
    "I will provide a sample implementation in Python using the Iris dataset.\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, :2]  # Using only the first two features for visualization\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into a training set and a testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train linear SVM using scikit-learn\n",
    "svm_clf = SVC(kernel='linear', C=1)  # You can adjust C for different regularization strengths\n",
    "svm_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict labels for the testing set\n",
    "y_pred = svm_clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Plot decision boundaries\n",
    "def plot_decision_boundary(X, y, model, title):\n",
    "    h = .02  # Step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
