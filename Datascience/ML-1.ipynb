{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1: Explain the following with an example:**\n",
    "- **Artificial Intelligence (AI):** Artificial Intelligence refers to the simulation of human intelligence in machines that are programmed to think, learn, and problem-solve like a human. It encompasses a broad range of techniques and technologies aimed at enabling machines to perform tasks that typically require human intelligence.\n",
    "\n",
    "  **Example:** Chatbots, like Siri or Alexa, use AI to understand and respond to natural language queries from users.\n",
    "\n",
    "- **Machine Learning (ML):** Machine Learning is a subset of AI that focuses on the development of algorithms that can learn from data and make predictions or decisions without being explicitly programmed. It involves the use of statistical techniques to enable machines to improve their performance on a specific task through learning from data.\n",
    "\n",
    "  **Example:** In spam email detection, ML algorithms can learn to classify emails as spam or not based on features extracted from past emails.\n",
    "\n",
    "- **Deep Learning (DL):** Deep Learning is a subfield of ML that uses artificial neural networks with many layers (deep neural networks) to model and solve complex tasks. It has been particularly successful in tasks involving image and speech recognition.\n",
    "\n",
    "  **Example:** Convolutional Neural Networks (CNNs) are a type of deep learning model widely used for image recognition tasks, such as identifying objects in photos.\n",
    "\n",
    "**Q2: What is supervised learning? List some examples of supervised learning.**\n",
    "\n",
    "- **Supervised Learning:** Supervised learning is a type of machine learning where the algorithm is trained on a labeled dataset, which means the input data is paired with corresponding output labels. The goal is for the algorithm to learn a mapping from input to output, making it capable of making predictions or classifications on unseen data.\n",
    "\n",
    "  **Examples of Supervised Learning:**\n",
    "  1. **Image Classification:** Given a dataset of images with labels (e.g., cats and dogs), a supervised learning algorithm can learn to classify new images into these categories.\n",
    "  2. **Spam Email Detection:** A supervised model can learn to classify emails as spam or not based on historical email data.\n",
    "  3. **Predicting House Prices:** Using features like square footage, number of bedrooms, and location, a supervised algorithm can predict the price of a house.\n",
    "\n",
    "**Q3: What is unsupervised learning? List some examples of unsupervised learning.**\n",
    "\n",
    "- **Unsupervised Learning:** Unsupervised learning is a type of machine learning where the algorithm is trained on an unlabeled dataset. The goal is to discover patterns, structures, or relationships within the data without explicit guidance in the form of output labels.\n",
    "\n",
    "  **Examples of Unsupervised Learning:**\n",
    "  1. **Clustering:** Unsupervised algorithms can group similar data points together. For example, clustering can be used to segment customers based on their purchase behavior without knowing in advance how many customer segments exist.\n",
    "  2. **Dimensionality Reduction:** Techniques like Principal Component Analysis (PCA) reduce the number of features in a dataset while preserving its important information.\n",
    "  3. **Anomaly Detection:** Identifying rare and unusual data points, such as fraud detection in financial transactions.\n",
    "\n",
    "**Q4: What is the difference between AI, ML, DL, and DS?**\n",
    "\n",
    "- **AI (Artificial Intelligence):** AI is the overarching field that aims to create machines or systems that can perform tasks requiring human-like intelligence.\n",
    "\n",
    "- **ML (Machine Learning):** ML is a subset of AI that focuses on developing algorithms that can learn from data and make predictions or decisions without explicit programming.\n",
    "\n",
    "- **DL (Deep Learning):** DL is a subfield of ML that uses deep neural networks to model and solve complex tasks, particularly well-suited for tasks involving unstructured data like images and text.\n",
    "\n",
    "- **DS (Data Science):** Data Science is a broader field that includes various techniques, including AI and ML, to extract insights and knowledge from data. It encompasses data collection, cleaning, analysis, and visualization.\n",
    "\n",
    "**Q5: What are the main differences between supervised, unsupervised, and semi-supervised learning?**\n",
    "\n",
    "- **Supervised Learning:** Requires a labeled dataset for training, with input-output pairs. The algorithm learns to map inputs to outputs and is used for tasks like classification and regression.\n",
    "\n",
    "- **Unsupervised Learning:** Uses unlabeled data for training and focuses on discovering patterns or structures within the data. Common tasks include clustering and dimensionality reduction.\n",
    "\n",
    "- **Semi-Supervised Learning:** Combines elements of both supervised and unsupervised learning. It uses a small amount of labeled data and a larger amount of unlabeled data for training. Semi-supervised learning can be beneficial when labeling data is expensive or time-consuming.\n",
    "\n",
    "**Q6: What is train, test, and validation split? Explain the importance of each term.**\n",
    "\n",
    "- **Training Data:** This is a subset of the dataset used to train the machine learning model. The model learns patterns and relationships from this data.\n",
    "\n",
    "- **Validation Data:** After training, the model is evaluated on the validation dataset to assess its performance. This helps in tuning hyperparameters and preventing overfitting.\n",
    "\n",
    "- **Test Data:** The test dataset is used to evaluate the final performance of the trained model. It provides an unbiased estimate of how well the model will perform on new, unseen data.\n",
    "\n",
    "The importance of each term:\n",
    "- **Training Data:** It's crucial for model learning and building a predictive model.\n",
    "- **Validation Data:** Helps fine-tune the model and prevent overfitting.\n",
    "- **Test Data:** Provides an unbiased assessment of the model's performance on new data, helping gauge its generalization ability.\n",
    "\n",
    "**Q7: How can unsupervised learning be used in anomaly detection?**\n",
    "\n",
    "Unsupervised learning can be used in anomaly detection by identifying data points that deviate significantly from the normal patterns present in the unlabeled dataset. Here's a general process:\n",
    "\n",
    "1. **Data Preparation:** Collect and preprocess the data, ensuring it's in a suitable format for unsupervised learning.\n",
    "\n",
    "2. **Unsupervised Learning Algorithm:** Apply unsupervised learning algorithms such as clustering (e.g., k-means) or dimensionality reduction (e.g., PCA) to learn the underlying patterns in the data.\n",
    "\n",
    "3. **Anomaly Detection:** After training, the model can identify data points that don't fit well within the learned patterns. Data points that deviate significantly from the majority are considered anomalies or outliers.\n",
    "\n",
    "4. **Thresholding:** Set a threshold to distinguish between normal and anomalous data points. Data points exceeding this threshold are flagged as anomalies.\n",
    "\n",
    "5. **Monitoring and Response:** Continuously monitor incoming data, and when anomalies are detected, take appropriate actions, such as notifying system administrators or triggering alerts.\n",
    "\n",
    "**Q8: List down some commonly used supervised learning algorithms and unsupervised learning algorithms. Explain in great detail with questions and format in such a way to look good on Google Colab text, kind like a README file.**\n",
    "\n",
    "Sure, here's a list of commonly used supervised and unsupervised learning algorithms along with brief explanations:\n",
    "\n",
    "### Supervised Learning Algorithms:\n",
    "\n",
    "1. **Linear Regression:**\n",
    "   - **Explanation:** Linear regression is used for predicting a continuous target variable based on one or more input features. It fits a linear relationship between the features and the target.\n",
    "   - **Questions:**\n",
    "     - What is the mathematical formula for a simple linear regression model?\n",
    "     - How do you handle multicollinearity in multiple linear regression?\n",
    "\n",
    "2. **Logistic Regression:**\n",
    "   - **Explanation:** Logistic regression is used for binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Q1. What is the Filter method in feature selection, and how does it work?\n",
    "\n",
    "**Filter Method Explanation:**\n",
    "The Filter method is a feature selection technique that assesses the relevance of each feature independently of the machine learning algorithm. It relies on statistical metrics to rank or score each feature based on its relationship with the target variable.\n",
    "\n",
    "**How it Works:**\n",
    "1. **Feature Scoring:** Each feature is assigned a score or ranking based on a statistical metric such as correlation, mutual information, chi-squared, or variance. The metric chosen depends on whether the target variable is categorical or continuous.\n",
    "\n",
    "2. **Selection Criteria:** Features are then selected based on their scores. You can set a threshold or choose the top N features to include in the model.\n",
    "\n",
    "**Pros:**\n",
    "- Fast and computationally efficient.\n",
    "- Does not require building a model.\n",
    "- Works well for datasets with a large number of features.\n",
    "\n",
    "**Cons:**\n",
    "- Ignores feature interactions.\n",
    "- May not consider the combined effect of features on the target variable.\n",
    "\n",
    "## Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
    "\n",
    "**Wrapper Method Explanation:**\n",
    "The Wrapper method, unlike the Filter method, evaluates feature subsets based on their impact on the performance of a specific machine learning algorithm. It involves building and evaluating multiple models with different feature subsets to find the best-performing set.\n",
    "\n",
    "**Differences:**\n",
    "1. **Evaluation:** Wrapper methods use a machine learning algorithm (e.g., decision tree, SVM) to evaluate feature subsets, while the Filter method relies on statistical metrics.\n",
    "\n",
    "2. **Feature Interaction:** Wrapper methods consider feature interactions since they evaluate subsets of features together, whereas Filter methods evaluate features independently.\n",
    "\n",
    "3. **Computationally Expensive:** Wrapper methods are computationally expensive as they involve training multiple models with different feature subsets. This makes them slower than Filter methods.\n",
    "\n",
    "## Q3. What are some common techniques used in Embedded feature selection methods?\n",
    "\n",
    "**Embedded Method Explanation:**\n",
    "Embedded feature selection methods incorporate feature selection into the model training process. They optimize feature selection as part of the model building. Common techniques include:\n",
    "\n",
    "1. **L1 Regularization (Lasso):** Penalizes the absolute magnitude of coefficients, encouraging some coefficients to be exactly zero, effectively selecting features.\n",
    "\n",
    "2. **Tree-Based Methods:** Decision trees and ensemble methods (e.g., Random Forest, Gradient Boosting) can rank features based on their importance scores, and you can select features accordingly.\n",
    "\n",
    "3. **Recursive Feature Elimination (RFE):** Involves iteratively fitting models and removing the least significant features until a specified number or criteria are met.\n",
    "\n",
    "4. **Feature Importance from Algorithms:** Some algorithms (e.g., XGBoost, LightGBM) provide built-in feature importance scores, which can be used for feature selection.\n",
    "\n",
    "## Q4. What are some drawbacks of using the Filter method for feature selection?\n",
    "\n",
    "**Drawbacks of Filter Method:**\n",
    "1. **Independence Assumption:** Filter methods treat features independently, ignoring potential interactions between features.\n",
    "\n",
    "2. **Suboptimal Feature Sets:** They may select suboptimal feature sets for specific machine learning algorithms since they do not consider algorithm-specific requirements.\n",
    "\n",
    "3. **No Model Feedback:** Filter methods do not involve the model-building process, so they may not capture the full predictive power of feature combinations.\n",
    "\n",
    "4. **Sensitivity to Thresholds:** The choice of threshold for feature selection can be arbitrary and affect the results.\n",
    "\n",
    "## Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n",
    "\n",
    "**Using Filter Method:**\n",
    "- When you have a large dataset with many features and want a quick initial feature selection step.\n",
    "- For exploratory data analysis and identifying potentially relevant features before building complex models.\n",
    "- When computational resources are limited, and you need an efficient method.\n",
    "\n",
    "## Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
    "\n",
    "**Filter Method for Telecom Churn Model:**\n",
    "1. **Data Exploration:** Begin by exploring the dataset to understand the features, their types (categorical or numerical), and the target variable (churn).\n",
    "\n",
    "2. **Feature Scoring:** Select a suitable feature scoring metric. For binary classification like churn prediction, you can use correlation (for numerical features) or chi-squared (for categorical features) as scoring metrics.\n",
    "\n",
    "3. **Score Calculation:** Calculate the score for each feature based on the chosen metric's relevance to churn.\n",
    "\n",
    "4. **Threshold Selection:** Set a threshold for feature selection. You can experiment with different threshold values and observe how many features are retained.\n",
    "\n",
    "5. **Feature Selection:** Select the features with scores above the chosen threshold to include in your churn prediction model.\n",
    "\n",
    "6. **Model Building:** Build a predictive model using the selected features (e.g., logistic regression or decision tree) and evaluate its performance.\n",
    "\n",
    "7. **Iterate:** You can iterate this process by trying different scoring metrics and thresholds to find the most pertinent attributes that improve the model's predictive power.\n",
    "\n",
    "## Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.\n",
    "\n",
    "**Embedded Method for Soccer Match Prediction:**\n",
    "1. **Data Preprocessing:** Begin by preprocessing the soccer match dataset, including handling missing values and encoding categorical features.\n",
    "\n",
    "2. **Feature Engineering:** Create additional relevant features if needed, such as player performance averages, team win streaks, or historical match outcomes.\n",
    "\n",
    "3. **Model Selection:** Choose a machine learning algorithm suitable for soccer match prediction, such as a decision tree, random forest, or gradient boosting.\n",
    "\n",
    "4. **Feature Importance:** Train the selected model and calculate feature importance scores. Most tree-based models provide feature importance scores as part of their output.\n",
    "\n",
    "5. **Feature Ranking:** Rank the features based on their importance scores. Features with higher scores are considered more relevant.\n",
    "\n",
    "6. **Feature Selection:** Select a subset of the most relevant features based on the ranking. You can experiment with different feature counts to optimize the model's performance.\n",
    "\n",
    "7. **Model Evaluation:** Evaluate the performance of the model using selected features, considering metrics like accuracy, precision, and recall.\n",
    "\n",
    "8. **Iterate:** If necessary, iterate the process by refining feature engineering or trying different algorithms to enhance prediction accuracy.\n",
    "\n",
    "## Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor.\n",
    "\n",
    "**Wrapper Method for House Price Prediction:**\n",
    "1. **Data Preprocessing:** Begin by preprocessing the house price dataset, including handling missing values, encoding categorical features, and scaling numerical features if necessary.\n",
    "\n",
    "2. **Feature Selection Space:** Define the space of possible feature subsets. Since you have a limited number of features, you can create all possible combinations of features.\n",
    "\n",
    "3. **Model Evaluation:** Choose a performance metric (e.g., Mean Absolute Error, Root Mean Squared Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly, I'll provide detailed explanations for each question along with examples where applicable:\n",
    "\n",
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
    "\n",
    "**Min-Max Scaling Explanation:**\n",
    "Min-Max scaling is a data preprocessing technique used to rescale numerical features to a specific range, typically between 0 and 1. It transforms each feature by subtracting the minimum value and dividing by the range (the difference between the maximum and minimum values).\n",
    "\n",
    "**Formula:** \n",
    "Min-Max Scaled Value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "**Example:**\n",
    "Suppose you have a dataset of house prices, and the 'area' feature ranges from 500 sq. ft. to 2500 sq. ft. Applying Min-Max scaling to this feature would transform it to a range between 0 and 1, making it easier for models to converge.\n",
    "\n",
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
    "\n",
    "**Unit Vector Scaling Explanation:**\n",
    "Unit Vector scaling, also known as normalization, scales each feature to have a magnitude of 1 while preserving the direction of the original data point. It is particularly useful when features have different units or scales. The formula divides each feature value by the Euclidean norm (magnitude) of the data point.\n",
    "\n",
    "**Formula:** \n",
    "Unit Vector = X / ||X||\n",
    "\n",
    "**Difference from Min-Max Scaling:**\n",
    "- Min-Max scaling rescales features to a specified range (e.g., [0, 1]), while Unit Vector scaling preserves direction and scales features to have magnitude 1.\n",
    "- Min-Max scaling is suitable for algorithms that rely on feature magnitudes, while Unit Vector scaling is useful when the direction of features is more critical.\n",
    "\n",
    "**Example:**\n",
    "In a dataset with 'height' in centimeters and 'weight' in kilograms, Unit Vector scaling ensures that both features have a magnitude of 1 while keeping their original direction.\n",
    "\n",
    "## Q3. What is PCA (Principal Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
    "\n",
    "**PCA Explanation:**\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to transform a dataset into a lower-dimensional space while preserving as much variance as possible. It achieves this by finding the principal components (linear combinations of original features) that explain the most variation in the data.\n",
    "\n",
    "**Example:**\n",
    "Suppose you have a dataset with features related to a person's height, weight, age, and income. By applying PCA, you can reduce these features into a smaller set of principal components that capture the essential information in the data while reducing dimensionality.\n",
    "\n",
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "**PCA and Feature Extraction Relationship:**\n",
    "PCA can be used for feature extraction, which means transforming the original features into a set of new features (principal components) that are a linear combination of the original features. These new features aim to capture the most important information in the data.\n",
    "\n",
    "**Example:**\n",
    "Consider an image dataset with thousands of pixel features. Applying PCA to this dataset can extract a reduced set of principal components that represent the most significant patterns or structures in the images. These principal components can serve as new features for downstream tasks like image classification.\n",
    "\n",
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
    "\n",
    "**Min-Max Scaling for Recommendation System:**\n",
    "1. **Identify Features:** Identify the numerical features in your dataset, such as 'price,' 'rating,' and 'delivery time.'\n",
    "\n",
    "2. **Apply Min-Max Scaling:** For each numerical feature, apply Min-Max scaling to rescale the values to the range [0, 1]. Use the Min-Max scaling formula: (X - X_min) / (X_max - X_min).\n",
    "\n",
    "3. **Updated Features:** Replace the original feature values with the scaled values. Now, all numerical features will have values in the [0, 1] range.\n",
    "\n",
    "4. **Normalization Purpose:** Min-Max scaling ensures that features with different scales (e.g., price in dollars and rating on a scale of 1 to 5) are on a consistent scale for modeling in the recommendation system.\n",
    "\n",
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
    "\n",
    "**PCA for Stock Price Prediction:**\n",
    "1. **Feature Selection:** Identify the relevant features related to stock price prediction, including company financial metrics and market trends.\n",
    "\n",
    "2. **Data Preprocessing:** Standardize the features to have zero mean and unit variance. This step is crucial for PCA.\n",
    "\n",
    "3. **Apply PCA:** Apply PCA to the preprocessed dataset. PCA will find the principal components that explain the most variance in the data.\n",
    "\n",
    "4. **Select Components:** Determine the number of principal components to retain. You can consider factors like the explained variance ratio and the desired dimensionality reduction.\n",
    "\n",
    "5. **Transform Data:** Transform the dataset using the selected principal components. This results in a reduced-dimension dataset with the most important information retained.\n",
    "\n",
    "6. **Model Building:** Use the reduced-dimension dataset for training and testing your stock price prediction model.\n",
    "\n",
    "PCA helps reduce the dimensionality of the dataset while preserving the most significant patterns or features that impact stock prices.\n",
    "\n",
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
    "\n",
    "**Min-Max Scaling Example:**\n",
    "1. Find the minimum and maximum values in the dataset: Min = 1, Max = 20.\n",
    "\n",
    "2. Apply Min-Max scaling to each value using the formula:\n",
    "   - Min-Max Scaled Value = (X - X_min) / (X_max - X_min)\n",
    "\n",
    "3. Transform each value:\n",
    "   - For X = 1: (-1) = (1 - 1) / (20 - 1)\n",
    "   - For X = 5: (-0.6) = (5 - 1) / (20 - 1)\n",
    "   - For X = 10: (0) = (10 - 1) / (20 - 1)\n",
    "   - For X = 15: (0.6) = (15 - 1) / (20 - 1)\n",
    "   - For X = 20: (1) = (20 - 1) / (20 - 1)\n",
    "\n",
    "Now, the values are scaled to the range of -1 to 1.\n",
    "\n",
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "**PCA Feature Extraction:**\n",
    "1. Standardize the features (e.g., height, weight, age, blood pressure) to have zero mean and unit variance.\n",
    "\n",
    "2. Apply PCA to the standardized dataset to find the principal components.\n",
    "\n",
    "3. Examine the explained variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Q1. Pearson Correlation Coefficient between Study Time and Exam Scores\n",
    "\n",
    "Suppose we have collected data on the study time (in hours) and final exam scores (out of 100) for a group of students:\n",
    "\n",
    "- Study Time: [10, 20, 15, 30, 25]\n",
    "- Exam Scores: [80, 90, 85, 95, 88]\n",
    "\n",
    "Let's calculate the Pearson correlation coefficient (r) using the formula:\n",
    "\n",
    "\\[\n",
    "r = \\frac{\\sum{(x_i - \\bar{x})(y_i - \\bar{y})}}{\\sqrt{\\sum{(x_i - \\bar{x})^2} \\sum{(y_i - \\bar{y})^2}}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(x_i\\) and \\(y_i\\) are individual data points.\n",
    "- \\(\\bar{x}\\) and \\(\\bar{y}\\) are the means of the data points.\n",
    "\n",
    "Calculating the values:\n",
    "- \\(\\bar{x} = \\frac{10+20+15+30+25}{5} = 20\\)\n",
    "- \\(\\bar{y} = \\frac{80+90+85+95+88}{5} = 87.6\\)\n",
    "\n",
    "Now, calculate the numerator:\n",
    "- Numerator = \\((10-20)(80-87.6) + (20-20)(90-87.6) + (15-20)(85-87.6) + (30-20)(95-87.6) + (25-20)(88-87.6) = -8.8\\)\n",
    "\n",
    "Calculate the denominators:\n",
    "- Denominator_x = \\(\\sqrt{(10-20)^2 + (20-20)^2 + (15-20)^2 + (30-20)^2 + (25-20)^2} = 15\\)\n",
    "- Denominator_y = \\(\\sqrt{(80-87.6)^2 + (90-87.6)^2 + (85-87.6)^2 + (95-87.6)^2 + (88-87.6)^2} \\approx 7.25\\)\n",
    "\n",
    "Now, calculate the Pearson correlation coefficient (r):\n",
    "- \\(r = \\frac{-8.8}{15 \\times 7.25} \\approx -0.085\\)\n",
    "\n",
    "Interpretation:\n",
    "The Pearson correlation coefficient between study time and exam scores is approximately -0.085. This value is close to zero, indicating a very weak linear relationship between the two variables. In other words, there is little to no linear correlation between the amount of time students spend studying and their final exam scores.\n",
    "\n",
    "## Q2. Spearman's Rank Correlation between Sleep and Job Satisfaction\n",
    "\n",
    "Suppose we have collected data on the amount of sleep (in hours) and job satisfaction levels (ranked on a scale of 1 to 10) for a group of individuals:\n",
    "\n",
    "- Sleep Hours: [7, 6, 8, 5, 7]\n",
    "- Job Satisfaction (Ranked): [6, 4, 7, 3, 6]\n",
    "\n",
    "Let's calculate Spearman's rank correlation using the formula:\n",
    "\n",
    "\\[\n",
    "\\rho = 1 - \\frac{6\\sum{d^2}}{n(n^2-1)}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\(d\\) is the difference between the ranks of corresponding pairs.\n",
    "- \\(n\\) is the number of data points.\n",
    "\n",
    "First, rank the data:\n",
    "- Sleep Hours: [3, 2, 4, 1, 3]\n",
    "- Job Satisfaction: [3, 2, 4, 1, 3]\n",
    "\n",
    "Now, calculate the differences and squared differences:\n",
    "- \\(d = [0, 0, 0, 0, 0]\\)\n",
    "- \\(d^2 = [0, 0, 0, 0, 0]\\)\n",
    "\n",
    "Calculate the numerator:\n",
    "- Numerator = \\(6 \\times \\sum{d^2} = 6 \\times (0 + 0 + 0 + 0 + 0) = 0\\)\n",
    "\n",
    "Calculate the denominator:\n",
    "- Denominator = \\(n(n^2-1) = 5(5^2-1) = 120\\)\n",
    "\n",
    "Now, calculate Spearman's rank correlation (\\(\\rho\\)):\n",
    "- \\(\\rho = 1 - \\frac{0}{120} = 1\\)\n",
    "\n",
    "Interpretation:\n",
    "The Spearman's rank correlation (\\(\\rho\\)) between sleep hours and job satisfaction is 1, indicating a perfect monotonic relationship. In this case, as the amount of sleep increases, job satisfaction tends to increase monotonically. The data points show a strong positive monotonic correlation.\n",
    "\n",
    "## Q3. Pearson and Spearman Correlation between Exercise Hours and BMI\n",
    "\n",
    "Suppose we have collected data on the number of hours of exercise per week and Body Mass Index (BMI) for 50 adults. Let's calculate both Pearson and Spearman correlations between these two variables and compare the results.\n",
    "\n",
    "Assuming the data is as follows:\n",
    "- Exercise Hours: [3, 2, 4, 5, 1, ...] (50 data points)\n",
    "- BMI: [25.4, 27.8, 24.2, 29.5, 26.1, ...] (50 data points)\n",
    "\n",
    "Calculating the Pearson correlation coefficient will measure the linear relationship between these variables, while calculating the Spearman rank correlation will measure the monotonic relationship. The results will depend on the actual data values.\n",
    "\n",
    "## Q4. Pearson Correlation between TV Hours and Physical Activity\n",
    "\n",
    "Suppose we have collected data on the number of hours individuals spend watching television per day and their level of physical activity (measured on a scale of 1 to 10) from a sample of 50 participants. Let's calculate the Pearson correlation coefficient between these two variables.\n",
    "\n",
    "Assuming the data is as follows:\n",
    "- TV Hours: [2, 3, 4, 5, 6, ...] (50 data points)\n",
    "- Physical Activity: [7, 6, 5, 4, 3, ...] (50 data points)\n",
    "\n",
    "We can calculate the Pearson correlation coefficient to measure the linear relationship between the number of TV hours and the level of physical activity.\n",
    "\n",
    "## Q5. Age and Soft Drink Preference\n",
    "\n",
    "The survey results show the relationship between age (in years) and soft drink preference (e.g., Coke, Pepsi, Mountain Dew). To analyze this relationship, we typically use methods such as chi-squared tests or contingency tables to assess the independence of age and soft drink preference. The format of the survey results provided doesn't directly lend itself to correlation coefficients.\n",
    "\n",
    "Please specify if you'd like further analysis or specific statistical tests applied to this data.\n",
    "\n",
    "## Q6. Pearson Correlation between Sales Calls and Sales\n",
    "\n",
    "To calculate the Pearson correlation coefficient between the number of sales calls made per day and the number of sales made per week for a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
