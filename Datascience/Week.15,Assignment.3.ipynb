{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "683fdf2b-f9b1-44e2-9aa1-576fe79dabfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Week.15\n",
    "#Assignment.3 \n",
    "#Question.1 : What is Ridge Regression, and how does it differ from ordinary least squares regression?\n",
    "#Answer.1 : # Ridge Regression vs. Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "# Ridge Regression:\n",
    "\n",
    "# - Ridge regression is a regularized linear regression technique that adds a penalty term to the ordinary least\n",
    "#squares (OLS) cost function.\n",
    "# - The penalty term is the sum of squared values of the coefficients, multiplied by a regularization parameter\n",
    "#(alpha or lambda).\n",
    "# - The goal of Ridge regression is to prevent overfitting by adding a regularization term that discourages large \n",
    "#coefficient values.\n",
    "\n",
    "# Cost Function for Ridge Regression:\n",
    "# - Cost = RSS (Residual Sum of Squares) + alpha * Σ(β_i^2)\n",
    "# - RSS measures the squared differences between predicted and actual values.\n",
    "# - The second term penalizes large coefficient values, controlled by the regularization parameter alpha.\n",
    "\n",
    "# Ordinary Least Squares (OLS) Regression:\n",
    "\n",
    "# - OLS regression is the standard linear regression technique, aiming to minimize the sum of squared differences \n",
    "#between observed and predicted values.\n",
    "# - It does not include any regularization term, and the coefficients are estimated directly from the training data \n",
    "#without modification.\n",
    "\n",
    "# Cost Function for OLS Regression:\n",
    "# - Cost = RSS (Residual Sum of Squares)\n",
    "# - OLS minimizes the squared differences between predicted and actual values without additional penalty terms.\n",
    "\n",
    "# Differences:\n",
    "\n",
    "# 1. Regularization:\n",
    "#    - Ridge regression includes a regularization term to prevent overfitting, while OLS does not incorporate any \n",
    "#regularization.\n",
    "\n",
    "# 2. Coefficient Shrinkage:\n",
    "#    - Ridge regression shrinks the coefficients towards zero but rarely forces them to be exactly zero.\n",
    "#    - OLS does not impose any penalty on coefficients, allowing them to take any value.\n",
    "\n",
    "# 3. Collinearity Handling:\n",
    "#    - Ridge regression is effective in handling multicollinearity (high correlation between predictors) by distributing \n",
    "#the impact of correlated variables.\n",
    "#    - OLS may struggle with multicollinearity, leading to unstable and imprecise coefficient estimates.\n",
    "\n",
    "# 4. Feature Importance:\n",
    "#    - Ridge regression retains all features in the model but with reduced weights, making it suitable for scenarios\n",
    "#where all features are relevant.\n",
    "#    - OLS may include all features with their original weights, potentially giving equal importance to less relevant features.\n",
    "\n",
    "# Example in Python:\n",
    "# - Implement Ridge regression using libraries like scikit-learn, specifying the alpha parameter to control the strength \n",
    "#of regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cbeefa-2b08-44f9-8046-4bcedddc13bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.2 : What are the assumptions of Ridge Regression?\n",
    "#Answer.2 : # Assumptions of Ridge Regression:\n",
    "\n",
    "# 1. Linearity:\n",
    "#    - Ridge regression assumes a linear relationship between the independent variables and the dependent variable.\n",
    "#    - The model aims to capture the linear trend in the data.\n",
    "\n",
    "# 2. Independence:\n",
    "#    - The residuals (differences between observed and predicted values) should be independent of each other.\n",
    "#    - Independence ensures that the errors do not follow a specific pattern or exhibit serial correlation.\n",
    "\n",
    "# 3. Homoscedasticity:\n",
    "#    - The variance of the residuals should be constant across all levels of the independent variables.\n",
    "#    - Homoscedasticity ensures that the spread of residuals remains consistent, indicating that the model's \n",
    "#predictive power is stable.\n",
    "\n",
    "# 4. Normality of Residuals:\n",
    "#    - Ridge regression does not require the normality of residuals, but it benefits from relatively normally\n",
    "#distributed errors.\n",
    "#    - Normality assumptions are less strict compared to classical linear regression.\n",
    "\n",
    "# 5. No Perfect Multicollinearity:\n",
    "#    - Ridge regression can handle multicollinearity, but extreme cases of perfect multicollinearity may still pose challenges.\n",
    "#    - Multicollinearity arises when independent variables are highly correlated with each other.\n",
    "\n",
    "# 6. Model Appropriateness:\n",
    "#    - Ridge regression assumes that the model chosen is appropriate for the data.\n",
    "#    - The model should reasonably represent the underlying relationships between variables.\n",
    "\n",
    "# 7. Stationarity:\n",
    "#    - Ridge regression does not explicitly assume stationarity, but it is essential to ensure that the relationships\n",
    "#between variables remain consistent over time.\n",
    "\n",
    "# Note:\n",
    "# - While Ridge regression relaxes some assumptions of classical linear regression, it is crucial to assess these assumptions\n",
    "#to ensure the reliability of the model.\n",
    "# - Regularization techniques like Ridge are often applied when there are concerns about multicollinearity and overfitting\n",
    "#in the presence of a large number of predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "183c40a2-f2a6-4cdf-ace8-11c5a0781920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.3 : How do you select the value of the tuning parameter (lambda) in Ridge Regression?\n",
    "#Answer.3 : # Selecting the Tuning Parameter (Lambda) in Ridge Regression:\n",
    "\n",
    "# 1. Cross-Validation:\n",
    "#    - Use cross-validation techniques, such as k-fold cross-validation, to evaluate the model's performance for \n",
    "#different values of lambda.\n",
    "#    - Split the dataset into k folds, train the model on k-1 folds, and validate on the remaining fold. Repeat for\n",
    "#different lambda values.\n",
    "\n",
    "# 2. Grid Search:\n",
    "#    - Perform a grid search over a range of lambda values to find the one that minimizes the validation error.\n",
    "#    - Specify a list of potential lambda values and evaluate the model's performance for each value.\n",
    "\n",
    "# 3. RidgeCV in scikit-learn:\n",
    "#    - Utilize RidgeCV in scikit-learn, which internally performs cross-validation to find the optimal alpha (equivalent \n",
    "#to lambda) value.\n",
    "#    - RidgeCV automates the process of selecting the best regularization parameter.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "# from sklearn.linear_model import RidgeCV\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# # Create RidgeCV model with a range of alpha values\n",
    "# alphas = [0.1, 1.0, 10.0]\n",
    "# ridge_cv_model = RidgeCV(alphas=alphas, store_cv_values=True)\n",
    "\n",
    "# # Fit the model on the training data\n",
    "# ridge_cv_model.fit(X_train, y_train)\n",
    "\n",
    "# # Access the optimal alpha value chosen by RidgeCV\n",
    "# optimal_alpha = ridge_cv_model.alpha_\n",
    "# print(f'Optimal Alpha (Lambda): {optimal_alpha}')\n",
    "\n",
    "# # Alternatively, use cross-validation scores to analyze the performance for different alpha values\n",
    "# cv_scores = ridge_cv_model.cv_values_\n",
    "# mean_cv_scores = np.mean(cv_scores, axis=0)\n",
    "# print(f'Mean Cross-Validation Scores for Alphas: {mean_cv_scores}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fce50a9-019c-4993-ace3-1c7254d2af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.4 : Can Ridge Regression be used for feature selection? If yes, how?\n",
    "#Answer.4 : # Ridge Regression for Feature Selection:\n",
    "\n",
    "# Yes, Ridge Regression can be used for feature selection, although it tends to retain all features in the model.\n",
    "\n",
    "# 1. L2 Regularization:\n",
    "#    - Ridge Regression uses L2 regularization, which adds a penalty term based on the sum of squared coefficients to \n",
    "#the cost function.\n",
    "\n",
    "# 2. Continuous Shrinkage:\n",
    "#    - Ridge regression shrinks the coefficients towards zero but rarely forces them to be exactly zero.\n",
    "\n",
    "# 3. Encouraging Small Coefficients:\n",
    "#    - The L2 penalty term encourages all coefficients to be small, but it does not promote sparsity.\n",
    "\n",
    "# 4. Ridge Regression Path:\n",
    "#    - The Ridge regression path involves plotting the coefficients against the regularization parameter (lambda).\n",
    "#    - As lambda increases, coefficients tend to shrink towards zero, but none are forced to become exactly zero.\n",
    "\n",
    "# 5. Feature Importance Ordering:\n",
    "#    - While Ridge does not provide automatic feature selection by setting coefficients to zero, it does indicate\n",
    "#the importance of features based on the magnitude of their coefficients.\n",
    "\n",
    "# 6. Limitations for Sparsity:\n",
    "#    - Ridge is generally not as effective for sparsity-inducing feature selection as Lasso (L1 regularization).\n",
    "#    - If exact feature sparsity is crucial, Lasso may be more suitable.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# # Create synthetic data\n",
    "# X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "\n",
    "# # Fit Ridge Regression model\n",
    "# ridge_model = Ridge(alpha=1.0)  # Adjust alpha (lambda) for desired regularization strength\n",
    "# ridge_model.fit(X, y)\n",
    "\n",
    "# # Access the coefficients after fitting\n",
    "# coefficients = ridge_model.coef_\n",
    "# print(f'Coefficients: {coefficients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ac76591-58c4-4661-bbb6-35b434cbc274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.5 : How does the Ridge Regression model perform in the presence of multicollinearity?\n",
    "#Answer.5 : # Ridge Regression and Multicollinearity:\n",
    "\n",
    "# Ridge Regression is effective in handling multicollinearity, a situation where independent variables are highly correlated.\n",
    "\n",
    "# 1. Objective of Ridge in Multicollinearity:\n",
    "#    - Ridge aims to distribute the impact of correlated variables by shrinking their coefficients proportionally.\n",
    "\n",
    "# 2. Reduction of Coefficient Magnitudes:\n",
    "#    - Ridge reduces the magnitudes of coefficients, preventing them from taking extreme values when multicollinearity \n",
    "#is present.\n",
    "\n",
    "# 3. Stability in Coefficient Estimates:\n",
    "#    - The inclusion of the L2 regularization term in Ridge helps stabilize coefficient estimates, making them less \n",
    "#sensitive to small changes in the data.\n",
    "\n",
    "# 4. Ridge Regression Path:\n",
    "#    - The Ridge regression path, which plots coefficients against the regularization parameter (lambda), shows the\n",
    "#behavior of coefficients as lambda increases.\n",
    "#    - As lambda increases, coefficients tend to shrink towards zero, providing a more balanced impact for correlated\n",
    "#variables.\n",
    "\n",
    "# 5. Advantage Over OLS:\n",
    "#    - Ridge performs better than Ordinary Least Squares (OLS) regression in the presence of multicollinearity, where OLS\n",
    "#may yield unstable and imprecise coefficient estimates.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# # Create synthetic data with multicollinearity\n",
    "# X, y = make_regression(n_samples=100, n_features=10, noise=0.1, random_state=42)\n",
    "# X[:, 5] = X[:, 0] * 2  # Introduce multicollinearity between features 0 and 5\n",
    "\n",
    "# # Fit Ridge Regression model\n",
    "# ridge_model = Ridge(alpha=1.0)  # Adjust alpha (lambda) for desired regularization strength\n",
    "# ridge_model.fit(X, y)\n",
    "\n",
    "# # Access the coefficients after fitting\n",
    "# coefficients = ridge_model.coef_\n",
    "# print(f'Coefficients: {coefficients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b55bccbb-8615-4ada-be74-e26c842c7d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.6 : Can Ridge Regression handle both categorical and continuous independent variables?\n",
    "#Answer.6 : # Ridge Regression and Variable Types:\n",
    "\n",
    "# Ridge Regression is versatile and can handle both continuous and categorical independent variables.\n",
    "\n",
    "# 1. Continuous Variables:\n",
    "#    - Ridge Regression naturally handles continuous variables by estimating coefficients for each continuous feature.\n",
    "\n",
    "# 2. Categorical Variables:\n",
    "#    - For categorical variables, they need to be appropriately encoded before applying Ridge Regression.\n",
    "#    - Common encoding techniques include one-hot encoding, label encoding, or other suitable methods based on the\n",
    "#nature of the categorical variables.\n",
    "\n",
    "# 3. One-Hot Encoding:\n",
    "#    - One-hot encoding is commonly used for categorical variables with more than two levels.\n",
    "#    - Each category is represented by a binary indicator column (0 or 1) in the dataset.\n",
    "\n",
    "# 4. Combining Continuous and Categorical Features:\n",
    "#    - After encoding, the dataset can consist of both continuous and binary-encoded categorical features.\n",
    "#    - Ridge Regression can then be applied to the combined dataset.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# # Create synthetic data with both continuous and categorical variables\n",
    "# X, y = make_regression(n_samples=100, n_features=5, n_informative=3, noise=0.1, random_state=42)\n",
    "\n",
    "# # Introduce a categorical variable (feature 0) with three levels\n",
    "# X[:, 0] = [0, 1, 2] * 33\n",
    "\n",
    "# # Define a ColumnTransformer to apply one-hot encoding to the categorical variable\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[('cat', OneHotEncoder(), [0])]\n",
    "# )\n",
    "\n",
    "# # Create a Ridge Regression model\n",
    "# ridge_model = Ridge(alpha=1.0)  # Adjust alpha (lambda) for desired regularization strength\n",
    "\n",
    "# # Create a pipeline to apply one-hot encoding and then fit the Ridge model\n",
    "# model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('ridge', ridge_model)])\n",
    "# model_pipeline.fit(X, y)\n",
    "\n",
    "# # Access the coefficients after fitting\n",
    "# coefficients = ridge_model.coef_\n",
    "# print(f'Coefficients: {coefficients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3bd1fc3-fb2f-41c2-9e95-d68fd8592f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.7 : How do you interpret the coefficients of Ridge Regression?\n",
    "#Answer.7 : # Interpreting Coefficients in Ridge Regression:\n",
    "\n",
    "# 1. Magnitude of Coefficients:\n",
    "#    - The magnitude of the coefficients indicates the strength of the relationship between each independent\n",
    "#variable and the dependent variable.\n",
    "\n",
    "# 2. Sign of Coefficients:\n",
    "#    - The sign of the coefficients (+ or -) indicates the direction of the relationship. Positive coefficients\n",
    "#suggest a positive correlation, and negative coefficients suggest a negative correlation.\n",
    "\n",
    "# 3. Impact of Regularization:\n",
    "#    - Ridge Regression introduces a penalty term to the cost function based on the sum of squared coefficients multiplied \n",
    "#by the regularization parameter (alpha or lambda).\n",
    "#    - Coefficients are shrunk towards zero, and the degree of shrinkage is controlled by the regularization parameter.\n",
    "\n",
    "# 4. Importance of Features:\n",
    "#    - Features with larger absolute coefficients have a stronger impact on the predictions.\n",
    "#    - The importance of features can be inferred based on the magnitude of their coefficients.\n",
    "\n",
    "# 5. Stability in Coefficient Estimates:\n",
    "#    - Ridge Regression stabilizes coefficient estimates, making them less sensitive to small changes in the data.\n",
    "#    - This is particularly useful in the presence of multicollinearity.\n",
    "\n",
    "# Example in Python:\n",
    "\n",
    "# from sklearn.linear_model import Ridge\n",
    "# from sklearn.datasets import make_regression\n",
    "\n",
    "# # Create synthetic data\n",
    "# X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
    "\n",
    "# # Fit Ridge Regression model\n",
    "# ridge_model = Ridge(alpha=1.0)  # Adjust alpha (lambda) for desired regularization strength\n",
    "# ridge_model.fit(X, y)\n",
    "\n",
    "# # Access the coefficients after fitting\n",
    "# coefficients = ridge_model.coef_\n",
    "# print(f'Coefficients: {coefficients}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e405d1-3cc0-4456-9621-d7b4909c55b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question.8 : Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
